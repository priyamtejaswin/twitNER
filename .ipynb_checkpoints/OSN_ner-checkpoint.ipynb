{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on OSN's\n",
    "\n",
    "**TASK**:Find the most popular entites(people, organizations, locations) on Facebook and Twitter for any event.\n",
    "\n",
    "1. Collect data from both networks on a common topic.\n",
    "2. Run NER on the data.\n",
    "3. Find intersecting entities and rank by total mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data collection\n",
    "\n",
    "I chose \"nba\" as the topic - the season just started, so I guessed there will be a significant amount of online activity.\n",
    "\n",
    "I used tweepy(easy_install tweepy) for collecting the 2500 most recent english tweets from twitter with the query string as \"nba\". Facebook no longer supports search across all public posts, so after consulting Yatharth, I took the 500 most recent posts from two pages - NBA and NBAtv.\n",
    "\n",
    "Run *test_tweepy.py* and *test_graph_api.py* for collecting data; you will need to create an additional file for access token called *AccessTokens.py* and store all api keys there. I used mongodb for storing raw data locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## named entity recognition\n",
    "\n",
    "I tried 2 methods initially:\n",
    "1. NLTK NER chunker.\n",
    "2. Stanford NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "tagger = NERTagger(\"stanford-ner-2014-06-16/classifiers/english.all.3class.distsim.crf.ser.gz\", \"stanford-ner-2014-06-16/stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "def stanford_ner(text):\n",
    "    print \"--using stanford--\"\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        ne_tagged_sent = tagger.tag(nltk.word_tokenize(sent))[0]\n",
    "        for tup in ne_tagged_sent:\n",
    "            if tup[1]!=\"O\":\n",
    "                print tup\n",
    "\n",
    "\n",
    "def nltk_ner(text):\n",
    "    print \"--using nltk--\"\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if len(chunk)==1:\n",
    "                print chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorothy Bland: I was caught 'walking while black' - Dallas Morning News https://t.co/EvcWF8pdss\n",
      "--using nltk--\n",
      "(PERSON Dorothy/NNP)\n",
      "(GPE Bland/NNP)\n",
      "--using stanford--\n",
      "(u'Dorothy', u'PERSON')\n",
      "(u'Bland', u'PERSON')\n",
      "(u'Dallas', u'LOCATION')\n",
      "\n",
      "----------\n",
      "\n",
      "NBA Trade Rumors: Chicago Bulls Moving Derrick Rose Within This Season?: Will point guard Derrick Rose https://t.co/aJf9rQ8bmn #Kpopstarz\n",
      "--using nltk--\n",
      "(ORGANIZATION NBA/NNP)\n",
      "(PERSON Season/NNP)\n",
      "(PERSON Will/NNP)\n",
      "--using stanford--\n",
      "(u'Chicago', u'ORGANIZATION')\n",
      "(u'Bulls', u'ORGANIZATION')\n",
      "(u'Derrick', u'PERSON')\n",
      "(u'Rose', u'PERSON')\n",
      "(u'Derrick', u'PERSON')\n",
      "(u'Rose', u'PERSON')\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"Dorothy Bland: I was caught 'walking while black' - Dallas Morning News https://t.co/EvcWF8pdss\", \n",
    "         \"NBA Trade Rumors: Chicago Bulls Moving Derrick Rose Within This Season?: Will point guard Derrick Rose https://t.co/aJf9rQ8bmn #Kpopstarz\"\n",
    "         ]\n",
    "for tweet in tweets:\n",
    "    print tweet\n",
    "    nltk_ner(tweet)\n",
    "    stanford_ner(tweet)\n",
    "    print \"\\n----------\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were pretty much consistent across posts. So I continued with the Stanford NER tagger. The tagger follows the BIO format, so I collected consecutive non-outside objects as a single phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(tagged_sent):\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in tagged_sent:\n",
    "        if (tag != \"O\"):\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Chicago Bulls', u'ORGANIZATION'), (u'Derrick Rose', u'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "ne_tagged_sent = tagger.tag(nltk.word_tokenize(tweets[1]))[0]\n",
    "named_entities = get_continuous_chunks(ne_tagged_sent)\n",
    "print [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every tweet and facebook post, I constructed entites and grouped them by entity type(PERSON, ORGANIZATION, LOCATION). If you have stored the data on mongodb, run *twitter_ner.py* and *facebook_ner.py* to perform NER. This will save two pickles with entites grouped by (PERSON, ORGANIZATION, LOCATION)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## popular and common entites\n",
    "\n",
    "Once I had the entities grouped by entity_type, I \"grouped\" together similar entites. Consider:\n",
    "- Stephen Curry, Curry, Steph\n",
    "- LeBron James, James\n",
    "- D Rose, Derric Rose, Rose\n",
    "\n",
    "I ran a loop across all entities for a entity_type. \n",
    "- The program searched for an exact match; \n",
    "- - if found, it increases the count of the entity by 1;\n",
    "- - else, it searches for the closest match above a certain threshold; \n",
    "- - - if found, it increases the count of the **closes_match** by 1;\n",
    "- - - else, it initializes the entity with a count of 1;\n",
    "\n",
    "For finding similar strings, I used **difflib.get_close_matches(n=1, cutoff=0.5)**. Difflib is a default library, the  idea behind its algorithm is to find the longest contiguous matching subsequence that contains no \"junk\" elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************Using Stanford NER**************************************\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "u'derrick rose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e31be7d19315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n********************************Using Stanford NER**************************************\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mfind_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-e31be7d19315>\u001b[0m in \u001b[0;36mfind_common\u001b[0;34m(fb, tw)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfb_popular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw_popular\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_popular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_popular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"--from facebook--\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_entities\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfb_popular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e31be7d19315>\u001b[0m in \u001b[0;36mfind_popular\u001b[0;34m(entity_dict)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_entities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpopular\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mpopular\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mclosest_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopular\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: u'derrick rose'"
     ]
    }
   ],
   "source": [
    "import pickle, difflib\n",
    "\n",
    "def find_popular(entity_dict):\n",
    "    popular = {}\n",
    "    for entity_type, list_of_entities in entity_dict.items():\n",
    "        popular[entity_type] = {}\n",
    "        for entity in list_of_entities:\n",
    "            if popular[entity_type].get(entity, None):\n",
    "                popular[entity_type][entity]+=1\n",
    "            else:\n",
    "                closest_match = difflib.get_close_matches(entity, set(popular[entity_type].keys()), n=1, cutoff=0.5)            \n",
    "                if len(closest_match)==1:\n",
    "                    popular[entity_type][closest_match[0]]+=1\n",
    "                else:\n",
    "                    popular[entity_type][entity] =1\n",
    "    return popular\n",
    "\n",
    "def find_common(fb, tw):\n",
    "    fb_popular, tw_popular = find_popular(fb), find_popular(tw)\n",
    "    print \"--from facebook--\"\n",
    "    for entity_type, list_of_entities in fb_popular.items():\n",
    "        print entity_type.upper(), sorted(list_of_entities.items(), key=lambda x:x[1], reverse=True)[:5], \"\\n\"\n",
    "\n",
    "    print \"--from twitter--\"\n",
    "    for entity_type, list_of_entities in tw_popular.items():\n",
    "        print entity_type.upper(), sorted(list_of_entities.items(), key=lambda x:x[1], reverse=True)[:5], \"\\n\"\n",
    "\n",
    "    print \"--common--\"\n",
    "    common_entity_types = set(fb_popular.keys()).intersection(set(tw_popular.keys()))\n",
    "    for entity_type in common_entity_types:\n",
    "        common_entities = set(fb_popular[entity_type].keys()).intersection(set(tw_popular[entity_type].keys()))\n",
    "        if len(common_entities)>0:\n",
    "            print entity_type.upper(), sorted([(e, fb_popular[entity_type][e]+tw_popular[entity_type][e]) for e in common_entities], reverse=True, key=lambda tup:tup[1])[:5]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fb = pickle.load(open(\"facebook_ners_pkl\", \"r\"))\n",
    "    tw = pickle.load(open(\"twitter_ners_pkl\", \"r\"))\n",
    "\n",
    "    print \"\\n********************************Using Stanford NER**************************************\\n\"\n",
    "    find_common(fb, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
