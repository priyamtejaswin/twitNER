{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on OSN's\n",
    "\n",
    "**TASK**:Find the most popular entites(people, organizations, locations) on Facebook and Twitter for any event.\n",
    "\n",
    "1. Collect data from both networks on a common topic.\n",
    "2. Run NER on the data.\n",
    "3. Find intersecting entities and rank by total count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data collection\n",
    "\n",
    "I chose \"nba\" as the topic - the season just started, so I guessed there will be a significant amount of online activity.\n",
    "\n",
    "I used tweepy(easy_install tweepy) for collecting the 2500 most recent english tweets from twitter with the query string as \"nba\". Facebook no longer supports search across all public posts, so after consulting Yatharth, I took the 500 most recent posts from two pages - NBA and NBAtv.\n",
    "\n",
    "Run *test_tweepy.py* and *test_graph_api.py* for collecting data; you will need to create an additional file for access token called *AccessTokens.py* and store all api keys there. I used mongodb for storing raw data locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## named entity recognition\n",
    "\n",
    "I tried 2 methods initially:\n",
    "1. NLTK NER chunker.\n",
    "2. Stanford NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "tagger = NERTagger(\"stanford-ner-2014-06-16/classifiers/english.all.3class.distsim.crf.ser.gz\", \"stanford-ner-2014-06-16/stanford-ner.jar\", encoding='utf-8')\n",
    "\n",
    "def stanford_ner(text):\n",
    "    print \"--using stanford--\"\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        ne_tagged_sent = tagger.tag(nltk.word_tokenize(sent))[0]\n",
    "        for tup in ne_tagged_sent:\n",
    "            if tup[1]!=\"O\":\n",
    "                print tup\n",
    "\n",
    "\n",
    "def nltk_ner(text):\n",
    "    print \"--using nltk--\"\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if len(chunk)==1:\n",
    "                print chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorothy Bland: I was caught 'walking while black' - Dallas Morning News https://t.co/EvcWF8pdss\n",
      "--using nltk--\n",
      "(PERSON Dorothy/NNP)\n",
      "(GPE Bland/NNP)\n",
      "--using stanford--\n",
      "(u'Dorothy', u'PERSON')\n",
      "(u'Bland', u'PERSON')\n",
      "(u'Dallas', u'LOCATION')\n",
      "\n",
      "----------\n",
      "\n",
      "NBA Trade Rumors: Chicago Bulls Moving Derrick Rose Within This Season?: Will point guard Derrick Rose https://t.co/aJf9rQ8bmn #Kpopstarz\n",
      "--using nltk--\n",
      "(ORGANIZATION NBA/NNP)\n",
      "(PERSON Season/NNP)\n",
      "(PERSON Will/NNP)\n",
      "--using stanford--\n",
      "(u'Chicago', u'ORGANIZATION')\n",
      "(u'Bulls', u'ORGANIZATION')\n",
      "(u'Derrick', u'PERSON')\n",
      "(u'Rose', u'PERSON')\n",
      "(u'Derrick', u'PERSON')\n",
      "(u'Rose', u'PERSON')\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"Dorothy Bland: I was caught 'walking while black' - Dallas Morning News https://t.co/EvcWF8pdss\", \n",
    "         \"NBA Trade Rumors: Chicago Bulls Moving Derrick Rose Within This Season?: Will point guard Derrick Rose https://t.co/aJf9rQ8bmn #Kpopstarz\"\n",
    "         ]\n",
    "for tweet in tweets:\n",
    "    print tweet\n",
    "    nltk_ner(tweet)\n",
    "    stanford_ner(tweet)\n",
    "    print \"\\n----------\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were pretty much consistent across posts. So I continued with the Stanford NER tagger. The tagger follows the BIO format, so I collected consecutive non-outside objects as a single phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(tagged_sent):\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in tagged_sent:\n",
    "        if (tag != \"O\"):\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Chicago Bulls', u'ORGANIZATION'), (u'Derrick Rose', u'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "ne_tagged_sent = tagger.tag(nltk.word_tokenize(tweets[1]))[0]\n",
    "named_entities = get_continuous_chunks(ne_tagged_sent)\n",
    "print [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every tweet and facebook post, I constructed entites and grouped them by entity type(PERSON, ORGANIZATION, LOCATION). If you have stored the data on mongodb, run *twitter_ner.py* and *facebook_ner.py* to perform NER. This will save two pickles with entites grouped by (PERSON, ORGANIZATION, LOCATION)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## popular and common entites\n",
    "\n",
    "Once I had the entities grouped by entity_type, I \"grouped\" together similar entites. Consider:\n",
    "- Stephen Curry, Curry, Steph\n",
    "- LeBron James, James\n",
    "- D Rose, Derric Rose, Rose\n",
    "\n",
    "I ran a loop across all entities for a entity_type. \n",
    "- The program searched for an exact match; \n",
    "- - if found, it increases the count of the entity by 1;\n",
    "- - else, it searches for the closest match above a certain threshold; \n",
    "- - - if found, it increases the count of the **closes_match** by 1;\n",
    "- - - else, it initializes the entity with a count of 1;\n",
    "\n",
    "For finding similar strings, I used **difflib.get_close_matches(n=1, cutoff=0.5)**. Difflib is a default library, the  idea behind its algorithm is to find the longest contiguous matching subsequence that contains no \"junk\" elements.\n",
    "\n",
    "Once I have popular entites from both networks, I add the counts for intersecting entities and sort by decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************Using Stanford NER**************************************\n",
      "\n",
      "--from facebook--\n",
      "PERSON [(u'stephen curry', 44), (u'kevin durant', 36), (u'james', 26), (u'anthony', 23), (u'russell westbrook', 18)] \n",
      "\n",
      "LOCATION [(u'oklahoma city thunder', 30), (u'chicago', 19), (u'orlando', 18), (u'miami', 14), (u'spain', 13)] \n",
      "\n",
      "ORGANIZATION [(u'nba', 407), (u'l.a. clippers', 71), (u'houston rockets', 49), (u'golden state warriors', 45), (u'chicago bulls', 27)] \n",
      "\n",
      "--from twitter--\n",
      "PERSON [(u'kevin durant', 157), (u'derrick rose', 116), (u'wade', 41), (u'stephen curry', 28), (u'butler', 28)] \n",
      "\n",
      "LOCATION [(u'usa', 23), (u'memphis', 22), (u'japan', 17), (u'ireland', 13), (u'arizona', 10)] \n",
      "\n",
      "ORGANIZATION [(u'nba', 410), (u'chicago bulls', 189), (u'nhl', 101), (u'hornets', 48), (u'sixers', 27)] \n",
      "\n",
      "--common--\n",
      "PERSON [(u'kevin durant', 193), (u'derrick rose', 133), (u'stephen curry', 72), (u'james', 52), (u'bradley beal', 24)]\n",
      "LOCATION [(u'oklahoma city thunder', 40), (u'memphis', 23), (u'washington', 11), (u'philadelphia', 10), (u'minnesota', 10)]\n",
      "ORGANIZATION [(u'nba', 817), (u'chicago bulls', 216), (u'golden state warriors', 63), (u'san antonio spurs', 22), (u'toronto raptors', 20)]\n"
     ]
    }
   ],
   "source": [
    "import pickle, difflib\n",
    "\n",
    "def find_popular(entity_dict):\n",
    "    popular = {}\n",
    "    for entity_type, list_of_entities in entity_dict.items():\n",
    "        popular[entity_type] = {}\n",
    "        for entity in list_of_entities:\n",
    "            if popular[entity_type].get(entity, None):\n",
    "                popular[entity_type][entity]+=1\n",
    "            else:\n",
    "                closest_match = difflib.get_close_matches(entity, set(popular[entity_type].keys()), n=1, cutoff=0.5)            \n",
    "                if len(closest_match)==1:\n",
    "                    popular[entity_type][closest_match[0]]+=1\n",
    "                else:\n",
    "                    popular[entity_type][entity] =1\n",
    "    return popular\n",
    "\n",
    "def find_common(fb, tw):\n",
    "    fb_popular, tw_popular = find_popular(fb), find_popular(tw)\n",
    "    print \"--from facebook--\"\n",
    "    for entity_type, list_of_entities in fb_popular.items():\n",
    "        print entity_type.upper(), sorted(list_of_entities.items(), key=lambda x:x[1], reverse=True)[:5], \"\\n\"\n",
    "\n",
    "    print \"--from twitter--\"\n",
    "    for entity_type, list_of_entities in tw_popular.items():\n",
    "        print entity_type.upper(), sorted(list_of_entities.items(), key=lambda x:x[1], reverse=True)[:5], \"\\n\"\n",
    "\n",
    "    print \"--common--\"\n",
    "    common_entity_types = set(fb_popular.keys()).intersection(set(tw_popular.keys()))\n",
    "    for entity_type in common_entity_types:\n",
    "        common_entities = set(fb_popular[entity_type].keys()).intersection(set(tw_popular[entity_type].keys()))\n",
    "        if len(common_entities)>0:\n",
    "            print entity_type.upper(), sorted([(e, fb_popular[entity_type][e]+tw_popular[entity_type][e]) for e in common_entities], reverse=True, key=lambda tup:tup[1])[:5]\n",
    "\n",
    "fb = pickle.load(open(\"facebook_ners_pkl\", \"r\"))\n",
    "tw = pickle.load(open(\"twitter_ners_pkl\", \"r\"))\n",
    "\n",
    "print \"\\n********************************Using Stanford NER**************************************\\n\"\n",
    "find_common(fb, tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter specific NER\n",
    "\n",
    "For his paper \"Named Entity Recognition in Tweets: An Experimental Study(2011)\", Alan Ritter made a twitter specific pos-tagger, event classifier. https://github.com/aritter/twitter_nlp . The repo contains the training data.\n",
    "\n",
    "The entity types are very elaborate(movie, person, sportsteam, company, geo, tv show). For some entity_types, it was suprisingly accurate, so I included its results as well. The same procedure was followed. Run *popular.py* to get the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************Using Alan Ritters twitter specific NER**************************************\n",
      "\n",
      "--from facebook--\n",
      "PRODUCT [(u'NBA', 35), (u'Dallas Mavericks L.A. Clippers', 4), (u'NBA Fantasy', 2), (u'NBA NBA 2K', 2), (u'Rio Olympics', 1)] \n",
      "\n",
      "FACILITY [(u'Hall of Famer', 4), (u'Naismith Memorial Basketball Hall of Fame', 4), (u'Great Wall', 2), (u'Brooklyn Nets', 2), (u'Tower Bridge', 1)] \n",
      "\n",
      "MOVIE [(u'Ring Night', 2), (u'SLAM', 2), (u'NBA LEAGUE PASS', 2), (u'Metta World Peace', 1), (u'LA Lakers', 1)] \n",
      "\n",
      "COMPANY [(u'NBA', 33), (u'ESPN', 4), (u'EA SPORTS', 3), (u'Facebook', 2), (u'HBO', 1)] \n",
      "\n",
      "SPORTSTEAM [(u'Golden State Warriors', 53), (u'L.A. Clippers', 42), (u'Houston Rockets', 32), (u'Atlanta Hawks Heat', 26), (u'Cleveland Cavaliers', 24)] \n",
      "\n",
      "NONE [(u'2-time NBA champion Jo Jo White', 1), (u'New Orleans Pelicans guard', 1)] \n",
      "\n",
      "BAND [(u'The Jet', 4), (u'Kristaps Porzingis', 3), (u'Flamengo', 3), (u'Association', 2), (u'BOSH', 1)] \n",
      "\n",
      "PERSON [(u'NBA', 103), (u'Stephen Curry', 41), (u'Kevin Durant', 28), (u'Carmelo Anthony', 20), (u'Harrison Barnes', 19)] \n",
      "\n",
      "TVSHOW [(u'NBA Family', 4), (u'Ring of Honor', 3), (u'Rookie Class', 2), (u'Open Court', 2), (u'Andrew Wiggins', 1)] \n",
      "\n",
      "OTHER [(u'NBA', 179), (u'L.A. Clippers', 9), (u'ESPN', 7), (u'Phoenix Suns Ring of Honor', 5), (u'NBA LEAGUE PASS', 5)] \n",
      "\n",
      "GEO [(u'Chicago', 16), (u'Shanghai', 12), (u'Orlando', 8), (u'Spain', 5), (u'Houston', 5)] \n",
      "\n",
      "EVENT [(u'season', 93), (u'lead', 69), (u'win', 57), (u'looking', 44), (u'time', 39)] \n",
      "\n",
      "--from twitter--\n",
      "PRODUCT [(u'NBA', 92), (u'NBA roundup', 6), (u'NBA Capsules', 4), (u'Scouts Guide', 3), (u'Xbox', 3)] \n",
      "\n",
      "FACILITY [(u'Vine', 7), (u'PANINI', 4), (u'NBA Philippines', 2), (u'Not Substance', 2), (u'NBA Barber', 2)] \n",
      "\n",
      "MOVIE [(u'Rose Gold', 3), (u'Shaqtin', 3), (u'NBA Crazy Bloopers', 3), (u'Burnt Orange', 2), (u'Good Xbox', 2)] \n",
      "\n",
      "COMPANY [(u'NBA', 123), (u'NFL', 11), (u'BD', 3), (u'ESPN', 3), (u'Black', 2)] \n",
      "\n",
      "SPORTSTEAM [(u'Chicago', 106), (u'NBA Trade', 68), (u'Cavs', 67), (u'Portland Trail Blazers', 37), (u'Thunder', 36)] \n",
      "\n",
      "NONE [(u'ganan Heat y Hornets', 2), (u'Damian Lillard', 1), (u'Celtics-Pacers Preview', 1), (u'NBA Capsules', 1), (u'Nuggets vs . Jazz Gallery', 1)] \n",
      "\n",
      "DAY [(u'B-Day', 1)] \n",
      "\n",
      "BAND [(u'The League', 9), (u'Thunder', 6), (u'Audio', 3), (u'Rockets-Kings Preview', 2), (u'Rajon Rondo Black Electric', 2)] \n",
      "\n",
      "PERSON [(u'NBA', 229), (u'Kevin Durant', 168), (u'Derrick Rose', 103), (u'Karl Towns', 62), (u'kobe', 57)] \n",
      "\n",
      "TVSHOW [(u'Inside The NBA', 13), (u'NBA Keeper League', 4), (u'NBA Finals', 3), (u'NBA Action : Timeout', 1), (u'Fox Boston', 1)] \n",
      "\n",
      "OTHER [(u'NBA', 68), (u'New York Times', 21), (u'NBA Khandwa', 20), (u'Bulls bury Thunder', 9), (u'NBC4 Washington', 7)] \n",
      "\n",
      "GEO [(u'Arizona', 23), (u'Trade', 12), (u'Portland', 9), (u'Japan', 8), (u'chicago', 7)] \n",
      "\n",
      "EVENT [(u'Ripped', 123), (u'Preventing', 64), (u'tilt', 57), (u'ranked', 50), (u'walking', 44)] \n",
      "\n",
      "--common--\n",
      "PRODUCT [(u'NBA', 127), (u'Bradley Beal', 2)]\n",
      "MOVIE [(u'SLAM', 3)]\n",
      "COMPANY [(u'NBA', 156), (u'ESPN', 7)]\n",
      "SPORTSTEAM [(u'Golden State Warriors', 75), (u'Thunder', 37), (u'Cleveland Cavaliers', 33), (u'Oklahoma City', 33), (u'Orlando Magic', 21)]\n",
      "PERSON [(u'NBA', 332), (u'Kevin Durant', 196), (u'Derrick Rose', 114), (u'Dwyane Wade', 43), (u'Russell Westbrook', 27)]\n",
      "OTHER [(u'NBA', 247), (u'ESPN', 10), (u'NBA History', 7), (u'TNT', 5), (u\"D'Angelo Russell\", 3)]\n",
      "GEO [(u'Memphis', 8), (u'Minnesota', 6), (u'Golden State', 5), (u'L.A.', 4), (u'London', 3)]\n",
      "EVENT [(u'victory', 37), (u'show', 28), (u'run', 18), (u'miss', 15), (u'Remember', 7)]\n"
     ]
    }
   ],
   "source": [
    "ritter_fb = pickle.load(open(\"ritter_fb_nes_pkl\", \"r\"))\n",
    "ritter_tw = pickle.load(open(\"ritter_tw_nes_pkl\", \"r\"))\n",
    "\n",
    "print \"\\n********************************Using Alan Ritters twitter specific NER**************************************\\n\"\n",
    "find_common(ritter_fb, ritter_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obligatory word-cloud(facebook, entity_type=PERSON)\n",
    "<img src=\"files/person.png\">\n",
    "\n",
    "Thats it. Let me know if this needs more work.\n",
    "\n",
    "Priyam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
